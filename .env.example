# OpenSearch (AWS) â€” hostname only (no https://)
OPENSEARCH_HOST=vpc-xxx.ap-south-1.es.amazonaws.com
OPENSEARCH_INDEX=wiki_kb_nested
OPENSEARCH_REGION=ap-south-1
AWS_ACCESS_KEY_ID=
AWS_SECRET_ACCESS_KEY=

# Input (path from repo root, or absolute)
# After ./data/download_streams.sh use:
# INPUT_FILE=data/streams
INPUT_FILE=//Users/riyamaurya/wiki_kb/streams
# Or any directory of .json.gz / .json.bz2 files:
# INPUT_FILE=/path/to/streams

# Optional: limit pages (leave empty for full dump)
# MAX_PAGES=10000

# ============================================================
# EMBEDDING PROVIDER: azure, gemma, or opensearch
# ============================================================
EMBEDDING_PROVIDER=opensearch

# Vector dimension (must match index schema and model output)
# msmarco-distilbert-base-tas-b: 768
# Azure text-embedding-3-small: 1536
# Gemma: 768 (or 512, 256, 128 via MRL truncation)
EMBEDDING_DIMENSION=768

# Max chunks to embed per document (for doc_vector mean pooling)
MAX_EMBEDDED_CHUNKS=3

# --- OpenSearch ML Embeddings (EMBEDDING_PROVIDER=opensearch) ---
# Use model deployed to OpenSearch ML Commons
OPENSEARCH_ML_MODEL_ID=your-deployed-model-id
# OPENSEARCH_ML_MAX_CHARS=2000

# --- Azure Embeddings (EMBEDDING_PROVIDER=azure) ---
# AZURE_EMBEDDING_ENDPOINT=https://your-resource.openai.azure.com/openai/deployments/your-deployment/embeddings
# AZURE_EMBEDDING_API_KEY=
# AZURE_EMBEDDING_API_VERSION=2024-02-01

# --- Gemma Embeddings (EMBEDDING_PROVIDER=gemma) ---
# Local inference (requires: uv sync --extra gemma)
# GEMMA_MODEL=google/embeddinggemma-300m
# GEMMA_DEVICE=cpu
# GEMMA_MAX_CHARS=2000

# Progress (resume). Default: output/<OPENSEARCH_INDEX>_pipeline_progress.json
# PROGRESS_FILE=output/wiki_kb_nested_pipeline_progress.json
